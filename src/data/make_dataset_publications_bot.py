# -*- coding: utf-8 -*-
"""make_dataset_publicacoes_bot.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-zv_1ZgFhSKo7EFJTxAE1Ljn3PHGewPq
"""

# -*- coding: utf-8 -*-
import click
import logging
from pathlib import Path
!pip install clean-text
from cleantext import clean
from nltk.tokenize import TweetTokenizer
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
from string import punctuation
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pandas as pd
import os
import json
from keras.models import model_from_json
import time
import datetime
!pip install python-dotenv
from dotenv import find_dotenv, load_dotenv


@click.command()
@click.argument('output_filepath', type=click.Path(), default = Path(__file__).resolve().parents[2])
def main(output_filepath):
    start_time = time.time()
    print("\nLoading countries's dataset...")
    """ Execute data processing scripts to gather country dataset data (../raw)
        and make them ready for sorting (saved in ../processed).
    """
    print('Loading data...')
    print('Loading csv files...')
    countries = os.listdir(r'{}\data\external\twitter'.format(project_dir))
    dict_df = {}
    for i in countries:
      dict_df[i] = pd.read_csv(r'{}\data\bronze\twitter_bronze\{}_raw_data.csv'.format(project_dir, "".join(list(i)[0:2]).upper()))
      dict_df[i]['country'] = i.capitalize()

    publications = pd.concat([dict_df[i] for i in list(dict_df.keys())])

    # removing emoji
    publications['text'] = publications['text'].apply(lambda x: clean(x, no_emoji=True))

    # removing stopwords and punctuation
    tt = TweetTokenizer()
    publications['text'] = publications['text'].apply(tt.tokenize)
    stopwords = set(stopwords.words('spanish') + list(punctuation))
    publications['text'] = publications['text'].apply(lambda x: ' '.join([word for word in x if word not in (stopwords)]))

    # padding the dataset
    train = pd.read_csv(r'{}\notebooks\data\train.csv'.format(project_dir))
    tk = Tokenizer()
    tk.fit_on_texts(train['review_es'].apply(str))
    tk_publications = tk.texts_to_sequences(publications['text'].apply(str))
    publications_pad = pad_sequences(tk_publications,padding="post",maxlen = 725)

    # adding the prediction dimensionality to the dataset
    # loading the model and its parameters
    json_file = open(r'{}\notebooks\data\model.json'.format(project_dir), 'r')
    loaded_model_json = json_file.read()
    json_file.close()
    loaded_model = model_from_json(loaded_model_json)
    # load weights into new model
    loaded_model.load_weights(r'{}\notebooks\data\model.h5'.format(project_dir))
    print("Loaded model from disk")
    
    # classifying the data
    prediction = loaded_model.predict(publications_pad)
    list_size = len(prediction)
    list1 = []
    for i in range(0,list_size):
      sum = 0
      for j in range(0,725):
        sum = predicao[i][j] + sum
      pontuacao = sum/725
      if pontuacao < 0.5:
        list1.append(0)
      else:
        list1.append(1)
    publications['prediction'] = list1

    # selecting the time range
    publications['date'] = pd.to_datetime(publications.date, format="%Y-%m-%d")
    date_start = datetime.datetime(2020, 2, 1)
    date_final = datetime.datetime(2020, 7, 31)
    df_analise = publications[(date_start <= publications['date']) &  (date_final > publications['date']) ]


    # saving the dataset
    df_analise.to_csv(r'{}\data\results_twitter\df_analise.csv'.format(output_filepath))
    print("Finishing Prediction's dataset...\n")
    print("Execution time: %s seconds \n" % (time.time() - start_time))
    

if __name__ == '__main__':
    log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    logging.basicConfig(level=logging.INFO, format=log_fmt)

    # not used in this stub but often useful for finding various files
    project_dir = Path(__file__).resolve().parents[2]

    # find .env automagically by walking up directories until it's found, then
    # load up the .env entries as environment variables
    load_dotenv(find_dotenv())

    main()