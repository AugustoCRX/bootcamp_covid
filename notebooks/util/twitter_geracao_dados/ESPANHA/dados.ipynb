{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import glob\n",
    "import json\n",
    "\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declara um Staticmethod, fazendo com que seja retornado um objeto da classe Language\n",
    "@Language.factory(\"language_detector\")\n",
    "#Cria a função para que seja voltado o objeto \"LanguageDetector()\"\n",
    "def get_lang_detector(nlp, name):\n",
    "   return LanguageDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LangDetector(text_nlp):\n",
    "    #Carrega modelo de leitura de detecção de linguagem\n",
    "    nlp = spacy.load('es_core_news_sm')\n",
    "    #Cria um pipeline com a função definida\n",
    "    nlp.add_pipe('language_detector',last=True)\n",
    "    text = text_nlp\n",
    "    doc = nlp(text)\n",
    "    return doc._.language['language'],doc._.language['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file):\n",
    "    with open(file, \"r\", encoding='utf8') as r:\n",
    "        response = r.read()\n",
    "        response = response.replace('\\n', '')\n",
    "        response = response.replace('}{', '},{')\n",
    "        response = \"[\" + response + \"]\"\n",
    "        return json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função de diminuição de dimensionalidade de listas\n",
    "#Caso o item seja uma lista, ele adiciona em uma lista vazia\n",
    "def flatten_list(_2d_list):\n",
    "    flat_list = []\n",
    "    for element in _2d_list:\n",
    "        if type(element) is list:\n",
    "            for item in element:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_ES = r\"DATA\"\n",
    "list_ES = glob.glob(os.path.join(caminho_ES, '*/*'))\n",
    "\n",
    "# extraindo json e criando lista\n",
    "dict_ES = list()\n",
    "for i in range(len(list_ES)):\n",
    "    dict_ES.append(read_json_file(list_ES[i]))\n",
    "dict_ES = flatten_list(dict_ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extração do \"place_id\" para coleta de dados geográficos\n",
    "data_extraction = []\n",
    "for i in range(len(dict_ES)):\n",
    "    try:\n",
    "        for j in range(len(dict_ES[i]['data'])):\n",
    "            place_id = dict_ES[i]['data'][j]['geo']['place_id']\n",
    "            data_extraction.append(place_id) \n",
    "    except:\n",
    "        continue  \n",
    "data_extraction = list(set(data_extraction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.listdir('GEO')\n",
    "dict_ES_GEO = list()\n",
    "for i in path:\n",
    "    with open(f'GEO/{i}', 'r', encoding='utf8') as f:\n",
    "        dict_ES_GEO.append(json.load(f))\n",
    "for i in range(len(dict_ES_GEO)):\n",
    "    try:\n",
    "        if dict_ES_GEO[i]['id'] in data_extraction:\n",
    "            data_extraction.remove(dict_ES_GEO[i]['id'])\n",
    "    except:\n",
    "        continue  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bot de extração de dados geográficos\n",
    "# # Para execução é necessário tirar os comentários do código\n",
    "# options = webdriver.ChromeOptions() #Execução do webdriver do google\n",
    "# options.add_experimental_option(\"prefs\", {\n",
    "#   \"download.default_directory\": r\"E:\\Blue\\Bootcamp 2\\github\\bootcamp_covid\\data_extraction\\ESPANHA\\GEO\"\n",
    "#   })\n",
    "# browser = webdriver.Chrome(options=options)\n",
    "# browser.get('https://twitter.com/')\n",
    "# time.sleep(2)\n",
    "# # Os elementos serão acessados via xpath, que é o path do HTML do site, buscando botões e caixa de interação\n",
    "# browser.find_element('xpath', '/html/body/div[1]/div/div/div[1]/div/div[1]/div/div/div/div/div/div/div/div[1]/a/div/span/span').click()\n",
    "# time.sleep(2)\n",
    "# # É seguro inserir seus dados, visto que o algoritmo só guardará seus dados na memória do seu computador\n",
    "# login = input('Insira seu nome no twitter (tag)') #Colocar como input\n",
    "# password = input('Insira sua senha') #Colocar como input\n",
    "# browser.find_element('xpath', '//html/body/div[1]/div/div/div[1]/div[2]/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div/div/div/div[5]/label/div/div[2]/div/input').send_keys(login)\n",
    "# time.sleep(2)\n",
    "# browser.find_element('xpath', '/html/body/div[1]/div/div/div[1]/div[2]/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div/div/div/div[6]/div').click()\n",
    "# time.sleep(2)\n",
    "# browser.find_element('xpath', '/html/body/div[1]/div/div/div[1]/div[2]/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div[1]/div/div/div[3]/div/label/div/div[2]/div[1]/input').send_keys(password)\n",
    "# time.sleep(2)\n",
    "# browser.find_element('xpath', '/html/body/div[1]/div/div/div[1]/div[2]/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div[2]/div/div[1]/div/div/div/div/span/span').click()\n",
    "# time.sleep(2)\n",
    "# # Um loop para acessar os dados do \"place_id\" que estão dentro da lista data_extraction\n",
    "# for place_id in data_extraction:  \n",
    "#   browser.get(f'https://api.twitter.com/1.1/geo/id/{place_id}.json')\n",
    "# # A API do twitter restringe o uso da coleta de \"place_id\" em 100 vezes/hora então esse tempo é calculado\n",
    "# # prevendo possíveis \"bugs\" e considerando o tempo limite por hora\n",
    "#   time.sleep(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.listdir('GEO')\n",
    "dict_ES_GEO = list()\n",
    "for i in path:\n",
    "    with open(f'GEO/{i}', 'r', encoding='utf8') as f:\n",
    "        dict_ES_GEO.append(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A estrutura da lista funciona com o seguinte padrão:\n",
    "# O primeiro objeto (dict_AR[i]) é o tweet de várias pessoas que falaram sobre o assunto covid naquele dia\n",
    "# Dentro do objeto citado anteriormente podemos acessar as seguintes informações:\n",
    "######### data #########\n",
    "# Inclui os dados do tweet em si \n",
    "######### includes #########\n",
    "# Inclui os dados do usuário que tweetou\n",
    "######### meta #########\n",
    "# Inclui informações sobre a pesquisa\n",
    "# Com isso, as informações que serão coletadas serão:\n",
    "# - Texto\n",
    "# - Data\n",
    "# - Quantidade de Retweet\n",
    "# - Quantidade de likes\n",
    "# - Grau inicial de polaridade (Utilizando a biblioteca TextBlob)\n",
    "# - Latitude\n",
    "# - Longitude\n",
    "# O grau inicial será levado como consideração como baseline inicial\n",
    "\n",
    "\n",
    "all_text = []\n",
    "for i in range(len(dict_ES)):\n",
    "    try:\n",
    "        for j in range(len(dict_ES[i]['data'])):\n",
    "            text = dict_ES[i]['data'][j]['text']\n",
    "            for tweet in [dict_ES[i]['data'][j]['text']]:\n",
    "                analysis = tb(tweet)\n",
    "                polarity = analysis.sentiment.polarity\n",
    "            retweet = dict_ES[i]['data'][j]['public_metrics']['retweet_count']\n",
    "            like = dict_ES[i]['data'][j]['public_metrics']['like_count']\n",
    "            date = dict_ES[i]['data'][j]['created_at'].split('T')[0]\n",
    "            for z in range(len(dict_ES_GEO)):\n",
    "                if dict_ES_GEO[z]['id'] == dict_ES[i]['data'][j]['geo']['place_id']:\n",
    "                    lat = dict_ES_GEO[z]['centroid'][1]\n",
    "                    long = dict_ES_GEO[z]['centroid'][0]\n",
    "            all_text.append(dict(text = text,\n",
    "                                score = polarity,\n",
    "                                retweet = retweet,\n",
    "                                like = like,\n",
    "                                date = date,\n",
    "                                lat = lat,\n",
    "                                long = long\n",
    "                                )) \n",
    "    except:\n",
    "        continue  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_df = pd.DataFrame(all_text)\n",
    "ES_df.to_csv('ES_data1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_df = pd.DataFrame(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Blue\\Bootcamp 2\\github\\bootcamp_covid\\data_extraction\\ESPANHA\\dados.ipynb Célula: 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Blue/Bootcamp%202/github/bootcamp_covid/data_extraction/ESPANHA/dados.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Lang_score \u001b[39m=\u001b[39m [LangDetector(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m ES_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "\u001b[1;32me:\\Blue\\Bootcamp 2\\github\\bootcamp_covid\\data_extraction\\ESPANHA\\dados.ipynb Célula: 14\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Blue/Bootcamp%202/github/bootcamp_covid/data_extraction/ESPANHA/dados.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Lang_score \u001b[39m=\u001b[39m [LangDetector(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m ES_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "\u001b[1;32me:\\Blue\\Bootcamp 2\\github\\bootcamp_covid\\data_extraction\\ESPANHA\\dados.ipynb Célula: 14\u001b[0m in \u001b[0;36mLangDetector\u001b[1;34m(text_nlp)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Blue/Bootcamp%202/github/bootcamp_covid/data_extraction/ESPANHA/dados.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mLangDetector\u001b[39m(text_nlp):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Blue/Bootcamp%202/github/bootcamp_covid/data_extraction/ESPANHA/dados.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m#Carrega modelo de leitura de detecção de linguagem\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Blue/Bootcamp%202/github/bootcamp_covid/data_extraction/ESPANHA/dados.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mes_core_news_sm\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Blue/Bootcamp%202/github/bootcamp_covid/data_extraction/ESPANHA/dados.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m#Cria um pipeline com a função definida\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Blue/Bootcamp%202/github/bootcamp_covid/data_extraction/ESPANHA/dados.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     nlp\u001b[39m.\u001b[39madd_pipe(\u001b[39m'\u001b[39m\u001b[39mlanguage_detector\u001b[39m\u001b[39m'\u001b[39m,last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32me:\\anaconda\\lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     37\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[0;32m     38\u001b[0m     \u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[0;32m     52\u001b[0m         name, vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, exclude\u001b[39m=\u001b[39;49mexclude, config\u001b[39m=\u001b[39;49mconfig\n\u001b[0;32m     53\u001b[0m     )\n",
      "File \u001b[1;32me:\\anaconda\\lib\\site-packages\\spacy\\util.py:420\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m get_lang_class(name\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mblank:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))()\n\u001b[0;32m    419\u001b[0m \u001b[39mif\u001b[39;00m is_package(name):  \u001b[39m# installed as package\u001b[39;00m\n\u001b[1;32m--> 420\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_package(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[39mif\u001b[39;00m Path(name)\u001b[39m.\u001b[39mexists():  \u001b[39m# path to model data directory\u001b[39;00m\n\u001b[0;32m    422\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda\\lib\\site-packages\\spacy\\util.py:453\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \n\u001b[0;32m    440\u001b[0m \u001b[39mname (str): The package name.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(name)\n\u001b[1;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mload(vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, exclude\u001b[39m=\u001b[39;49mexclude, config\u001b[39m=\u001b[39;49mconfig)\n",
      "File \u001b[1;32me:\\anaconda\\lib\\site-packages\\es_core_news_sm\\__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[1;34m(**overrides)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moverrides):\n\u001b[1;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_init_py(\u001b[39m__file__\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moverrides)\n",
      "File \u001b[1;32me:\\anaconda\\lib\\site-packages\\spacy\\util.py:619\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[1;34m(init_file, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_path\u001b[39m.\u001b[39mexists():\n\u001b[0;32m    618\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE052\u001b[39m.\u001b[39mformat(path\u001b[39m=\u001b[39mdata_path))\n\u001b[1;32m--> 619\u001b[0m \u001b[39mreturn\u001b[39;00m load_model_from_path(\n\u001b[0;32m    620\u001b[0m     data_path,\n\u001b[0;32m    621\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m    622\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[0;32m    623\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m    624\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m    625\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m    626\u001b[0m )\n",
      "File \u001b[1;32me:\\anaconda\\lib\\site-packages\\spacy\\util.py:488\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    486\u001b[0m overrides \u001b[39m=\u001b[39m dict_to_dot(config)\n\u001b[0;32m    487\u001b[0m config \u001b[39m=\u001b[39m load_config(config_path, overrides\u001b[39m=\u001b[39moverrides)\n\u001b[1;32m--> 488\u001b[0m nlp \u001b[39m=\u001b[39m load_model_from_config(\n\u001b[0;32m    489\u001b[0m     config, vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, exclude\u001b[39m=\u001b[39;49mexclude, meta\u001b[39m=\u001b[39;49mmeta\n\u001b[0;32m    490\u001b[0m )\n\u001b[0;32m    491\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\u001b[39m.\u001b[39mfrom_disk(model_path, exclude\u001b[39m=\u001b[39mexclude, overrides\u001b[39m=\u001b[39moverrides)\n",
      "File \u001b[1;32me:\\anaconda\\lib\\site-packages\\spacy\\util.py:528\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[1;34m(config, meta, vocab, disable, exclude, auto_fill, validate)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[39m# This will automatically handle all codes registered via the languages\u001b[39;00m\n\u001b[0;32m    526\u001b[0m \u001b[39m# registry, including custom subclasses provided via entry points\u001b[39;00m\n\u001b[0;32m    527\u001b[0m lang_cls \u001b[39m=\u001b[39m get_lang_class(nlp_config[\u001b[39m\"\u001b[39m\u001b[39mlang\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m--> 528\u001b[0m nlp \u001b[39m=\u001b[39m lang_cls\u001b[39m.\u001b[39;49mfrom_config(\n\u001b[0;32m    529\u001b[0m     config,\n\u001b[0;32m    530\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m    531\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m    532\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m    533\u001b[0m     auto_fill\u001b[39m=\u001b[39;49mauto_fill,\n\u001b[0;32m    534\u001b[0m     validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    535\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[0;32m    536\u001b[0m )\n\u001b[0;32m    537\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\n",
      "File \u001b[1;32me:\\anaconda\\lib\\site-packages\\spacy\\language.py:1779\u001b[0m, in \u001b[0;36mLanguage.from_config\u001b[1;34m(cls, config, vocab, disable, exclude, meta, auto_fill, validate)\u001b[0m\n\u001b[0;32m   1773\u001b[0m warn_if_jupyter_cupy()\n\u001b[0;32m   1775\u001b[0m \u001b[39m# Note that we don't load vectors here, instead they get loaded explicitly\u001b[39;00m\n\u001b[0;32m   1776\u001b[0m \u001b[39m# inside stuff like the spacy train function. If we loaded them here,\u001b[39;00m\n\u001b[0;32m   1777\u001b[0m \u001b[39m# then we would load them twice at runtime: once when we make from config,\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m \u001b[39m# and then again when we load from disk.\u001b[39;00m\n\u001b[1;32m-> 1779\u001b[0m nlp \u001b[39m=\u001b[39m lang_cls(vocab\u001b[39m=\u001b[39;49mvocab, create_tokenizer\u001b[39m=\u001b[39;49mcreate_tokenizer, meta\u001b[39m=\u001b[39;49mmeta)\n\u001b[0;32m   1780\u001b[0m \u001b[39mif\u001b[39;00m after_creation \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1781\u001b[0m     nlp \u001b[39m=\u001b[39m after_creation(nlp)\n",
      "File \u001b[1;32me:\\anaconda\\lib\\site-packages\\spacy\\language.py:190\u001b[0m, in \u001b[0;36mLanguage.__init__\u001b[1;34m(self, vocab, max_length, meta, create_tokenizer, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m     tokenizer_cfg \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtokenizer\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config[\u001b[39m\"\u001b[39m\u001b[39mnlp\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtokenizer\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m    189\u001b[0m     create_tokenizer \u001b[39m=\u001b[39m registry\u001b[39m.\u001b[39mresolve(tokenizer_cfg)[\u001b[39m\"\u001b[39m\u001b[39mtokenizer\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m create_tokenizer(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    191\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m batch_size\n\u001b[0;32m    192\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_error_handler \u001b[39m=\u001b[39m raise_error\n",
      "File \u001b[1;32me:\\anaconda\\lib\\site-packages\\spacy\\language.py:92\u001b[0m, in \u001b[0;36mcreate_tokenizer.<locals>.tokenizer_factory\u001b[1;34m(nlp)\u001b[0m\n\u001b[0;32m     90\u001b[0m suffix_search \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mcompile_suffix_regex(suffixes)\u001b[39m.\u001b[39msearch \u001b[39mif\u001b[39;00m suffixes \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m infix_finditer \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mcompile_infix_regex(infixes)\u001b[39m.\u001b[39mfinditer \u001b[39mif\u001b[39;00m infixes \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m \u001b[39mreturn\u001b[39;00m Tokenizer(\n\u001b[0;32m     93\u001b[0m     nlp\u001b[39m.\u001b[39;49mvocab,\n\u001b[0;32m     94\u001b[0m     rules\u001b[39m=\u001b[39;49mnlp\u001b[39m.\u001b[39;49mDefaults\u001b[39m.\u001b[39;49mtokenizer_exceptions,\n\u001b[0;32m     95\u001b[0m     prefix_search\u001b[39m=\u001b[39;49mprefix_search,\n\u001b[0;32m     96\u001b[0m     suffix_search\u001b[39m=\u001b[39;49msuffix_search,\n\u001b[0;32m     97\u001b[0m     infix_finditer\u001b[39m=\u001b[39;49minfix_finditer,\n\u001b[0;32m     98\u001b[0m     token_match\u001b[39m=\u001b[39;49mnlp\u001b[39m.\u001b[39;49mDefaults\u001b[39m.\u001b[39;49mtoken_match,\n\u001b[0;32m     99\u001b[0m     url_match\u001b[39m=\u001b[39;49mnlp\u001b[39m.\u001b[39;49mDefaults\u001b[39m.\u001b[39;49murl_match,\n\u001b[0;32m    100\u001b[0m )\n",
      "File \u001b[1;32me:\\anaconda\\lib\\site-packages\\spacy\\tokenizer.pyx:75\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32me:\\anaconda\\lib\\site-packages\\spacy\\tokenizer.pyx:574\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._load_special_cases\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32me:\\anaconda\\lib\\site-packages\\spacy\\tokenizer.pyx:618\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32me:\\anaconda\\lib\\site-packages\\spacy\\tokenizer.pyx:169\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32me:\\anaconda\\lib\\site-packages\\spacy\\tokens\\doc.pyx:232\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32me:\\anaconda\\lib\\site-packages\\spacy\\tokens\\_dict_proxies.py:29\u001b[0m, in \u001b[0;36mSpanGroups.__init__\u001b[1;34m(self, doc, items)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     27\u001b[0m     \u001b[39mself\u001b[39m, doc: \u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m, items: Iterable[Tuple[\u001b[39mstr\u001b[39m, SpanGroup]] \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m()\n\u001b[0;32m     28\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdoc_ref \u001b[39m=\u001b[39m weakref\u001b[39m.\u001b[39mref(doc)\n\u001b[0;32m     30\u001b[0m     UserDict\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, items)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Lang_score = [LangDetector(i) for i in ES_df['text']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7758e92e9a61d7a3490898707f7eeb937c85e9d1e8d4e877cc6c187218f226d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
