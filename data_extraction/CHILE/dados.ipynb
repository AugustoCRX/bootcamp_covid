{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import glob\n",
    "import json\n",
    "\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 58045)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32me:\\Blue\\Bootcamp 2\\github\\bootcamp_covid\\data_extraction\\CHILE\\dados.ipynb Célula: 2\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Blue/Bootcamp%202/github/bootcamp_covid/data_extraction/CHILE/dados.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(list_AR)):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Blue/Bootcamp%202/github/bootcamp_covid/data_extraction/CHILE/dados.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(list_AR[i], \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Blue/Bootcamp%202/github/bootcamp_covid/data_extraction/CHILE/dados.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         dict_AR\u001b[39m.\u001b[39mappend(json\u001b[39m.\u001b[39;49mload(f))\n",
      "File \u001b[1;32me:\\anaconda\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(fp, \u001b[39m*\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_float\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_pairs_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m     \u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[39m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39mread(),\n\u001b[0;32m    294\u001b[0m         \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m, object_hook\u001b[39m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[39m=\u001b[39mparse_float, parse_int\u001b[39m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[39m=\u001b[39mparse_constant, object_pairs_hook\u001b[39m=\u001b[39mobject_pairs_hook, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32me:\\anaconda\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32me:\\anaconda\\lib\\json\\decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n\u001b[1;32m--> 340\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExtra data\u001b[39m\u001b[39m\"\u001b[39m, s, end)\n\u001b[0;32m    341\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 58045)"
     ]
    }
   ],
   "source": [
    "caminho_AR = r\"DATA\"\n",
    "list_AR = glob.glob(os.path.join(caminho_AR, '*/*'))\n",
    "\n",
    "# extraindo json e criando lista\n",
    "dict_AR = list()\n",
    "for i in range(len(list_AR)):\n",
    "    with open(list_AR[i], 'r', encoding='utf8') as f:\n",
    "        dict_AR.append(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extração do \"place_id\" para coleta de dados geográficos\n",
    "data_extraction = []\n",
    "for i in range(len(dict_AR)):\n",
    "    try:\n",
    "        for j in range(len(dict_AR[i]['data'])):\n",
    "            place_id = dict_AR[i]['data'][j]['geo']['place_id']\n",
    "            data_extraction.append(place_id) \n",
    "    except:\n",
    "        continue  \n",
    "data_extraction = list(set(data_extraction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bot de extração de dados geográficos\n",
    "# Para execução é necessário tirar os comentários do código\n",
    "options = webdriver.ChromeOptions() #Execução do webdriver do google\n",
    "options.add_experimental_option(\"prefs\", {\n",
    "  \"download.default_directory\": r\"E:\\Blue\\Bootcamp 2\\github\\bootcamp_covid\\data_extraction\\CHILE\\GEO\"\n",
    "  })\n",
    "browser = webdriver.Chrome(options=options)\n",
    "browser.get('https://twitter.com/')\n",
    "time.sleep(2)\n",
    "# Os elementos serão acessados via xpath, que é o path do HTML do site, buscando botões e caixa de interação\n",
    "browser.find_element('xpath', '/html/body/div[1]/div/div/div[1]/div/div[1]/div/div/div/div/div/div/div/div[1]/a/div/span/span').click()\n",
    "time.sleep(2)\n",
    "# É seguro inserir seus dados, visto que o algoritmo só guardará seus dados na memória do seu computador\n",
    "login = input('Insira seu nome no twitter (tag)') #Colocar como input\n",
    "password = input('Insira sua senha') #Colocar como input\n",
    "browser.find_element('xpath', '//html/body/div[1]/div/div/div[1]/div[2]/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div/div/div/div[5]/label/div/div[2]/div/input').send_keys(login)\n",
    "time.sleep(2)\n",
    "browser.find_element('xpath', '/html/body/div[1]/div/div/div[1]/div[2]/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div/div/div/div[6]/div').click()\n",
    "time.sleep(2)\n",
    "browser.find_element('xpath', '/html/body/div[1]/div/div/div[1]/div[2]/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div[1]/div/div/div[3]/div/label/div/div[2]/div[1]/input').send_keys(password)\n",
    "time.sleep(2)\n",
    "browser.find_element('xpath', '/html/body/div[1]/div/div/div[1]/div[2]/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div[2]/div/div[1]/div/div/div/div/span/span').click()\n",
    "time.sleep(2)\n",
    "# Um loop para acessar os dados do \"place_id\" que estão dentro da lista data_extraction\n",
    "for place_id in data_extraction:  \n",
    "  browser.get(f'https://api.twitter.com/1.1/geo/id/{place_id}.json')\n",
    "# A API do twitter restringe o uso da coleta de \"place_id\" em 100 vezes/hora então esse tempo é calculado\n",
    "# prevendo possíveis \"bugs\" e considerando o tempo limite por hora\n",
    "  time.sleep(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.listdir('GEO')\n",
    "dict_AR_GEO = list()\n",
    "for i in path:\n",
    "    with open(f'GEO/{i}', 'r', encoding='utf8') as f:\n",
    "        dict_AR_GEO.append(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A estrutura da lista funciona com o seguinte padrão:\n",
    "# O primeiro objeto (dict_AR[i]) é o tweet de várias pessoas que falaram sobre o assunto covid naquele dia\n",
    "# Dentro do objeto citado anteriormente podemos acessar as seguintes informações:\n",
    "######### data #########\n",
    "# Inclui os dados do tweet em si \n",
    "######### includes #########\n",
    "# Inclui os dados do usuário que tweetou\n",
    "######### meta #########\n",
    "# Inclui informações sobre a pesquisa\n",
    "# Com isso, as informações que serão coletadas serão:\n",
    "# - Texto\n",
    "# - Data\n",
    "# - Quantidade de Retweet\n",
    "# - Quantidade de likes\n",
    "# - Grau inicial de polaridade (Utilizando a biblioteca TextBlob)\n",
    "# O grau inicial será levado como consideração como baseline inicial\n",
    "\n",
    "# all_text = []\n",
    "# for i in range(len(dict_AR)):\n",
    "#     try:\n",
    "#         for j in range(len(dict_AR[i]['data'])):\n",
    "#             text = dict_AR[i]['data'][j]['text']\n",
    "#             for tweet in [dict_AR[i]['data'][j]['text']]:\n",
    "#                 analysis = tb(tweet)\n",
    "#                 polarity = analysis.sentiment.polarity\n",
    "#             retweet = dict_AR[i]['data'][j]['public_metrics']['retweet_count']\n",
    "#             like = dict_AR[i]['data'][j]['public_metrics']['like_count']\n",
    "#             date = dict_AR[i]['data'][j]['created_at'].split('T')[0]\n",
    "#             all_text.append(dict(text = text,\n",
    "#                                 score = polarity,\n",
    "#                                 retweet = retweet,\n",
    "#                                 like = like,\n",
    "#                                 date = date\n",
    "#                                 )) \n",
    "#     except:\n",
    "#         continue  \n",
    "\n",
    "all_text = []\n",
    "for i in range(len(dict_AR)):\n",
    "    try:\n",
    "        for j in range(len(dict_AR[i]['data'])):\n",
    "            text = dict_AR[i]['data'][j]['text']\n",
    "            for tweet in [dict_AR[i]['data'][j]['text']]:\n",
    "                analysis = tb(tweet)\n",
    "                polarity = analysis.sentiment.polarity\n",
    "            retweet = dict_AR[i]['data'][j]['public_metrics']['retweet_count']\n",
    "            like = dict_AR[i]['data'][j]['public_metrics']['like_count']\n",
    "            date = dict_AR[i]['data'][j]['created_at'].split('T')[0]\n",
    "            for z in range(len(dict_AR_GEO)):\n",
    "                if dict_AR_GEO[z]['id'] == dict_AR[i]['data'][j]['geo']['place_id']:\n",
    "                    lat = dict_AR_GEO[0]['centroid'][1]\n",
    "                    long = dict_AR_GEO[0]['centroid'][0]\n",
    "            all_text.append(dict(text = text,\n",
    "                                score = polarity,\n",
    "                                retweet = retweet,\n",
    "                                like = like,\n",
    "                                date = date,\n",
    "                                lat = lat,\n",
    "                                long = long\n",
    "                                )) \n",
    "    except:\n",
    "        continue  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR_df = pd.DataFrame(all_text)\n",
    "AR_df.to_csv('AR_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7758e92e9a61d7a3490898707f7eeb937c85e9d1e8d4e877cc6c187218f226d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
